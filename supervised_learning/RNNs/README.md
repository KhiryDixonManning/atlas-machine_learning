The point of this folder is to create and run various types of Recurrent Neural Networks. Recurrent neural networks
traditionally are used to help predict an output or several outputs in sequential data sets. The example that is
used most often is to predict that next word in a sentence. Various other examples exist to show how a RNN can
be a useful tool. Simple RNN's exist and would be useful for things like suggesting the next word in a simple sentence.
When you need more complexity the world of RNN's can offer more complexity with three different types of RNN's:

Deep RNN's: 
An RNN with multiple stacked layers, allowing it to learn more complex patterns.
GRU:
A type of RNN that uses gates to control the flow of information, helping it learn long-range dependencies.
LTSM:
A type of RNN with gates and a cell state, designed to capture long-range dependencies in sequences.

We will create instances of each type and run them with the hope of showcasing the capabilities
of RNNs.